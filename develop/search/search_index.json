{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Boa Study Template","text":"<p>Boa is a domain-specific language and infrastructure that eases mining software repositories. Boa's infrastructure leverages distributed computing techniques to execute queries against hundreds of thousands of software projects very efficiently.</p> <p>Boa's Study Template was designed to aid researchers who want to use Boa in a research study.  The goal was two-fold: first, to make it easier to write, run, and process Boa queries and outputs, and second, to enable better replication of research.</p> <p>To accomplish these goals, the study template automatically manages a lot of the details such as when to run a Boa query, when to download its results, and automatic conversion of the output to CSV.  Additionally, the study template provides some helper functions in Python to make it easier to analyze that data.  Generally, researchers using the study template simply need to write their Boa queries, write the Python analyses to process the output, and then run <code>make</code> to execute the study.  The study template figures out what queries need to be run, what outputs need to be downloaded, and what analyses need to be run based on what files have changed since the last run.</p> <p>Moreover, the study template has reproducibility in mind.  It automatically records information about the Boa jobs that were run and stores the data in a converted, compressed Parquet file.  It also provides <code>make</code> targets to produce replication package ZIP files that contain all the data and analyses needed to reproduce the study.  This includes the Boa queries, the Python analyses, the data, and the generated figures and tables.</p> <p>The study template also provides support for publishing your replication package on Zenodo.  This is a great way to get a DOI for your replication package and to make it easy for others to cite your work.  The study template will automatically generate a Zenodo metadata file and upload the replication package to Zenodo.  By default, it will support double-blinded submissions and provides an easy way to unblind your replication after your paper is accepted.</p>"},{"location":"installing/","title":"Installing","text":"<p>Read below for instructions on installing the Boa Study Template.  Note that the instructions will differ a bit depending on whether you are starting a new study or using a replication package.</p>"},{"location":"installing/#boa-api-credentials","title":"Boa API Credentials","text":"<p>Boa API Access for New Studies</p> <p>If you are starting a new research study and utilizing Boa's study template, a username/password to the Boa website and API are required.  See the full list of requirements for more details.</p> <p>Boa API Access for Replication Packages</p> <p>If you are using a replication package that utilized Boa's study template, you should not need any Boa API credentials (unless you wish to re-run the Boa queries in the package or intend to make changes to the study).</p> <p>Once you have a Boa API login, edit the file <code>.env</code> and enter your username as <code>BOA_API_USER='&lt;username here&gt;'</code>, then save and close the file.</p> <p>We recommend also setting your password in your OS keyring, so you are not prompted to enter it each time a download triggers.  To do this, first install the <code>keyring</code> Python package and then run:</p> <pre><code>keyring set boaapi &lt;username&gt;\n</code></pre> <p>where <code>&lt;username&gt;</code> is your Boa username.</p> <p>If you are unable to use the keyring, you can also enter your password into the <code>.env</code> file as <code>BOA_API_PW='&lt;password here&gt;'</code>.</p> <p>Finally, if the script can't find your username and/or password, it will prompt you for it.  Note however it will prompt for each file it downloads.</p>"},{"location":"installing/#using-a-replication-package","title":"Using a Replication Package","text":"<p>Most likely, the only thing you need to do to use a replication package is to unzip the <code>replication-pkg.zip</code> file (or equivalent).  This will create a directory with the Boa queries, the Python analyses, and the generated figures and tables.</p> <p>If you also wish to re-run the analyses, you will need the <code>data-cache.zip</code> file (or equivalent).  Unzip this file starting in the root of the replication package and it will place files into the <code>data/parquet/</code> folder.</p> <p>If you wish to generate new analyses, you may need the raw data.  You will need the <code>data.zip</code> file (or equivalent).  Unzip this file starting in the root of the replication package and it will place files into the <code>data/txt/</code> folder.</p>"},{"location":"installing/#starting-a-new-study","title":"Starting a new Study","text":"<p>To start a new study, first clone the study template from GitHub:</p> <pre><code>git clone https://github.com/boalang/study-template.git\n</code></pre> <p>Then follow our guide on using the template in a research study.</p>"},{"location":"replications/","title":"Using a Replication","text":"<p>This section of the documentation is for people using a replication package built with the Boa Study Template.</p> <p>If you downloaded a replication package that utilizes the Boa Study Template, please continue reading for more details on how to use that package.</p> <p>We strongly encourage you to utilize Visual Studio Code with the Boa Language and Infrastructure Extension installed.  If you have that extension, you can avoid running terminal commands (like <code>make</code>) and instead simply open the <code>study-config.json</code> file to control what files you download, which analyses you run, etc.</p> <p>If you plan to extend the replication package, beyond simply re-generating the paper's figures and tables, you will want to read up on Using the Template.</p>"},{"location":"replications/#requirements","title":"Requirements","text":"<p>You need a GNU Make compatible build system.  Tested on GNU Make 3.81, but should work with newer versions.</p> <p>If you plan to re-run any of the analyses, you will also need Python.  See the Python requirements for more information.</p> <p>Note that the Boa jobs themselves should be marked public, so you do not need a Boa user to view the actual jobs/output via the website.  However, the Boa API requires a user/password to use it, so programmatically downloading (even public jobs) currently requires authenticating.  You can, however, manually download each of the query outputs from the public URLs.</p>"},{"location":"replications/#docker-support","title":"Docker Support","text":"<p>If you have Docker installed, you can use the provided <code>Dockerfile</code> to build an image capable of running all the scripts.  This is the easiest way to get a working environment.  To build and run the image, run:</p> <pre><code>make run-docker\n</code></pre> <p>Once inside the container, you can run the <code>make reproduce</code> command to re-generate the figures and tables from the original paper.  This will use the cached data and should avoid having to download output from Boa.</p>"},{"location":"replications/#file-organization","title":"File Organization","text":"<p>The organization of a replication package is the same as in the original study. See the page on Paths for more information.</p>"},{"location":"replications/#getting-boa-output","title":"Getting Boa Output","text":"<p>NOTE: This step is only needed if you don't already have the output downloaded!  If you downloaded <code>data.zip</code>, you can skip this step!</p> <p>The first step is to run Boa queries to generate output TXT data for further processing: <code>make txt</code></p>"},{"location":"replications/#processing-the-boa-output","title":"Processing the Boa Output","text":"<p>The Boa output is in a custom format, so first we convert it all into standard CSV format: <code>make csvs</code></p> <p>If you use the <code>make data</code> command instead of manually obtaining the outputs, you do not need to do anything else as it will call this target for you.</p>"},{"location":"replications/#generating-figures-and-tables","title":"Generating Figures and Tables","text":"<p>To generate all the figures and tables for the paper, you need to run the analysis for each specific research question on the output from Boa.  There is also a helper target to run all analyses:</p> <pre><code>make analysis\n</code></pre> <p>Note that this triggers download of any missing Boa query outputs.  Or you can run all analyses on only the cached data:</p> <pre><code>make reproduce\n</code></pre> <p>If you want to run individual analyses, you can also do so.  The specific target names will vary based on the specific replication package, but often they are named based on research question:</p> <pre><code>make rq1\n</code></pre> <pre><code>make rq2\n</code></pre> <p>You can also run a single analysis on the cached data by adding <code>-reproduce</code> to the target name:</p> <pre><code>make rq1-reproduce\n</code></pre> <pre><code>make rq2-reproduce\n</code></pre>"},{"location":"requirements/","title":"Requirements","text":"<p>You need a GNU Make compatible build system.  Tested on GNU Make 3.81.</p>"},{"location":"requirements/#boa-api-access","title":"Boa API Access","text":"<p>A username/password to the Boa website and API are required to use the study template.  You can request one here: https://boa.cs.iastate.edu/request/</p>"},{"location":"requirements/#python-requirements","title":"Python Requirements","text":"<p>Python 3 is required for all scripts.  Tested on Python 3.9.12.</p> <p>See <code>requirements.txt</code> for the package requirements.  To install them, run:</p> <pre><code>pip3 install -r requirements.txt\n</code></pre> <p>There are also optional packages you can install to improve the experience.</p>"},{"location":"requirements/#optional-python-requirements","title":"Optional Python Requirements","text":"<p>If you install <code>tqdm</code>, some scripts will display a progress bar.  This can be very useful when processing extremely large (multi-GB) output files.</p> <p>If you install <code>keyring</code>, you can store your API credentials in your OS's keyring.  This is more secure than storing it in plaintext in the <code>.env</code> file.  If you don't utilize either of those options, it will prompt you for your username/password (once for each output it tries to download!).</p> <p>See <code>requirements-optional.txt</code> for the package requirements.  To install them, run:</p> <pre><code>pip3 install -r requirements-optional.txt\n</code></pre>"},{"location":"vscode/","title":"VSCode Extension","text":"<p>Boa provides a Visual Studio Code extension on the marketplace and the Open VSX Registry.</p> <p>Boa API Access</p> <p>While you can use this extension to view and write Boa queries, most functionality (like submitting and managing jobs) requires having a user/password on the Boa infrastructure. If you do not yet have one, please request a user.</p>"},{"location":"vscode/#features","title":"Features","text":""},{"location":"vscode/#viewing-and-writing-boa-queries","title":"Viewing and Writing Boa Queries","text":"<p>This extension provides support for viewing and writing Boa queries:</p> <p></p> <p>This includes things like syntax highlighting, the ability to insert useful code snippets, code completion, and support for study template substitutions (templates).</p> <p>Any substitutions in the <code>study-config.json</code> are automatically highlighted by the editor.  If you hover over one, you can see the value it will be replaced with.  The code completion also includes these substitutions.</p> <p>Note that since the editor does not know the context of which query you are editing, it will show all possible substitutions that could match.  If you want to view the file for a particular query, you can click the preview button in the top right of the editor and select the query you want to preview it for.</p>"},{"location":"vscode/#submitting-and-managing-boa-jobs","title":"Submitting and Managing Boa Jobs","text":"<p>The extensions also provides support for submitting queries to Boa and managing existing jobs:</p> <p></p> <p>This allows you to effectively stay in the IDE and never need to go to the Boa website.</p>"},{"location":"vscode/#using-boas-study-template","title":"Using Boa's Study Template","text":"<p>If you are using the study template, either for a new study or a replication package, the extension provides additional support for managing the study:</p> <p></p> <p>Simply open the <code>study-config.json</code> file and you will see several code actions available to you that allow you to download specific outputs, clean data, run specific analyses, etc.  This is an alternative to running <code>make</code> on the command line.  There is also a tree view that shows the various outputs and analyses that can be run and allows clicking on it to run them.</p> <p>Most of the filenames shown in the file can also be clicked on to open the file in the editor.</p>"},{"location":"vscode/#extension-settings","title":"Extension Settings","text":"<p>The extension will automatically prompt you for your Boa API credentials the first time you use it.</p> <p>This extension also contributes the following settings:</p> <ul> <li><code>boalang.dataset.favorite</code>: Favorite Boa dataset</li> <li><code>boalang.joblist.pagesize</code>: Number of jobs to show in the jobs tree view</li> <li><code>boalang.output.size</code>: Size (in bytes) to limit displaying query outputs</li> <li><code>boalang.joblist.autoload</code>: Should the jobs tree view refresh on extension activation?</li> </ul>"},{"location":"research/","title":"Using the Template","text":"<p>If you downloaded a replication package that utilizes the study template, please see the section on Using a Replication.</p> <p>This section of the documentation is intended for researchers using the study template to perform initial research.</p>"},{"location":"research/add-analysis/","title":"Adding Analyses","text":"<p>There is one sample analysis given in <code>analyses/rq1.py</code>. This relies on the sample query given in <code>boa/queries/rq1.boa</code> and makes use of de-duplication, which relies (indirectly) on the query given in <code>boa/queries/hashes.boa</code>.  This analysis will generate a single result, the table in <code>tables/kotlin/rq1.tex</code>.</p> <p>The steps to add a new analysis are as follows:</p> <ol> <li>Create a new Boa query (e.g., <code>foo.boa</code>):<ol> <li>Store the file in <code>boa/queries/</code> - note you can create sub-folders here if you want.</li> <li>Add any missing common snippets into <code>boa/snippets/</code>.</li> <li>Add the query into the <code>study-config.json</code>.</li> <li>Repeat Step 1 as many times as necessary.</li> </ol> </li> <li>Create a new Python script to analyze the data (e.g., <code>foo.py</code>) in the <code>analyses/</code> directory.</li> <li>Add a new entry to the <code>study-config.json</code> in the <code>analyses</code> object.<ol> <li>The name of the entry is the script filename (e.g., <code>foo.py</code>, without the <code>analyses/</code> prefix).</li> <li>Add an <code>input</code> key, that is an array of CSV filenames that the analysis depends on.</li> </ol> </li> </ol> <p>Once this is done, you should be able to run <code>make foo</code> (the target is the name of the script, without the file extension) to run the analysis task, or run <code>make analysis</code> to run all analysis tasks.</p> <p>Analyses can be disabled by setting the <code>disabled</code> key to true.  This will prevent the analysis from being run by <code>make analysis</code>, but will still allow <code>make foo</code>.</p>"},{"location":"research/add-query/","title":"Adding Queries","text":"<p>All information about what queries to run and where to download output is stored in the <code>study-config.json</code> file.  This file is used to automatically create a <code>Makefile.study</code> with make targets for each output file.</p> <p>All Boa queries must reside under the <code>boa/</code> folder.  There are two sub-folders under the query folder: <code>boa/queries/</code> and <code>boa/snippets/</code>.  <code>boa/queries/</code> is where you store most of your Boa queries.  A few re-usable queries and examples are already provided there.</p> <p>The <code>boa/snippets/</code> folder is where the query template system looks for any included files.  The template system is described in the next section.</p>"},{"location":"research/add-query/#study-configjson-schema","title":"study-config.json Schema","text":"<p>The most important file for your study is the <code>study-config.json</code> configuration file.  This file defines all datasets, all queries (and their output file paths), and any template substitutions needed to build the queries.</p>"},{"location":"research/add-query/#defining-datasets","title":"Defining Datasets","text":"study-config.json<pre><code>  \"datasets\": {\n    \"kotlin\": \"2021 Aug/Kotlin\",\n    \"python\": \"2021 Aug/Python\",\n    \"python-ds\": \"2020 August/Python-DS\",\n    \"java\": \"2019 October/GitHub\",\n    \"java-sf\": \"2013 September/SF\",\n    \"original\": \"2012 July/SF\"\n  },\n</code></pre> <p>The first thing to include is at least one dataset.  Datasets are listed in the <code>datasets</code> object.  Each dataset must have a unique <code>custom_name</code>.  This is the name used to reference the dataset in a query.  The value is the name of the Boa dataset, as on the Boa website.  You can list as many datasets here as you want.</p> <p>Listing datasets here makes it easy to repeat a study on a new dataset, as you only need to update a single entry in the JSON file and all queries will pick up the change.</p>"},{"location":"research/add-query/#defining-queries","title":"Defining Queries","text":"<p>The next object in the study config is the <code>queries</code> object:</p> study-config.json<pre><code>  \"queries\": {\n    \"kotlin/hashes.txt\": {\n      \"query\": \"queries/hashes.boa\",\n      \"dataset\": \"kotlin\",\n      \"processors\": {\n        \"gendupes.py\": {\n          \"output\": \"data/txt/kotlin/dupes.txt\",\n          \"csv\": \"kotlin/dupes.csv\",\n          \"cacheclean\": [\n            \"kotlin/*-deduped.parquet\"\n          ]\n        }\n      }\n    },\n    \"kotlin/rq1.txt\": {\n      \"query\": \"queries/rq1.boa\",\n      \"dataset\": \"kotlin\",\n      \"csv\": {\n        \"output\": \"kotlin/rq1.csv\",\n        \"test\": [\n          \"3,\\\\.kts?$\"\n        ]\n      }\n    },\n    \"kotlin/project-count.txt\": {\n      \"query\": \"queries/project-count.boa\",\n      \"dataset\": \"kotlin\",\n      \"csv\": {\n        \"output\": \"kotlin/project-count.csv\",\n        \"drop\": [\n          1\n        ]\n      }\n    },\n    \"python/project-count.txt\": {\n      \"query\": \"queries/project-count.boa\",\n      \"dataset\": \"python\",\n      \"csv\": {\n        \"output\": \"python/project-count.csv\",\n        \"drop\": [\n          1\n        ]\n      }\n    }\n  },\n</code></pre> <p>This object contains output filenames as keys.  Note that all output here is coming from Boa queries, so they are always TXT files stored under <code>data/txt/</code>.  You omit that prefix in the file paths listed here. You can include sub-folders if you want.</p> <p>The values must have, at a minimum, <code>query</code> and <code>dataset</code> keys listing the path to the Boa query stored under <code>boa/</code> (again, you omit that prefix here, but you can point to any path under that folder) and the name of a dataset defined earlier.</p> <p>If for some reason you don't want the Boa job to be marked public, you can set the <code>public</code> key to <code>false</code>.  By default, all submitted jobs will be marked public after submission.</p> <p>A query can also indicate if it should be converted to CSV format.  The output of most queries will probably need to be converted to CSV, so you can easily load the data into Pandas for analysis. This is indicated by adding a <code>csv</code> key. The value is either a string listing the output path for the CSV file (stored in <code>data/csv/</code>, with the prefix omitted here) or an object listing the <code>output</code> path and some optional parameters:</p> <ul> <li><code>test</code> (can be repeated)<ul> <li>Add a <code>\"column,test\"</code> pair, where the given column keeps consuming the row   until the given regex test matches. This is useful because the output from   Boa does not escape, so if a column (other than the last) contains strings   and if those strings wind up having <code>][</code> in them (as some filenames do),   the conversion script might break and create a jagged CSV table.</li> </ul> </li> <li><code>drop</code> (can be repeated)<ul> <li>Drop a column (0-indexed) when converting.</li> </ul> </li> <li><code>header</code><ul> <li>A header row to prepend to the CSV output.  Can be useful if you think   others might use the generated CSV files outside your own analyses.</li> </ul> </li> <li><code>index</code><ul> <li>Number of indices in the Boa output - if not given, infers from the first   line.  This is usually not needed.</li> </ul> </li> </ul> <p>Finally, a query can also indicate if the <code>gendupes.py</code> script should run on the output file.  This is used for queries that output file hashes from Boa, to allow identifying duplicate files (based on matching AST hashes) for later de-duplication during analysis.  The value takes the <code>output</code> path where to store the generated TXT file with duplicate hash data (here, you must provide the <code>data/txt/</code> prefix).  It can also provide an optional <code>csv</code> key to convert the generated TXT file into CSV format (with the prefix omitted).  An optional <code>cacheclean</code> key allows listing additional cache (Parquet) files to clean up when re-generating this output.</p>"},{"location":"research/add-query/#defining-substitutions","title":"Defining Substitutions","text":"study-config.json<pre><code>  \"substitutions\": [\n    {\n      \"target\": \"{@escape@}\",\n      \"file\": \"escape.boa\"\n    },\n    {\n      \"target\": \"{@project-filter@}\",\n      \"replacement\": \"if (input.stars &gt;= 5)\"\n    }\n  ],\n</code></pre> <p>The next object is the list of global template substitutions.  These substitutions are available to every query, but can be overridden by specific queries.  See templates for more details.</p>"},{"location":"research/add-query/#defining-analyses","title":"Defining Analyses","text":"study-config.json<pre><code>  \"analyses\": {\n    \"rq1.py\": {\n      \"input\": [\n        \"kotlin/rq1.csv\",\n        \"kotlin/dupes.csv\"\n      ]\n    }\n  }\n</code></pre> <p>The last object is the list of analysis scripts you want to run.  See the section on adding an analysis for more details.</p>"},{"location":"research/building/","title":"Building","text":"<p>To build, run <code>make</code>.  This will submit all queries, download their outputs, convert them (where necessary), and run all analysis tasks to generate the output tables/figures.</p> <p>As Boa queries are submitted, they are marked public (unless specified not to) and details about the submitted Boa job are cached in <code>jobs.json</code>. This file contains keys that are the name of an output TXT file (without the <code>data/txt/</code> prefix).  The values are the <code>job</code> number and a <code>sha256</code> hash of the source query (after running through the template engine). The hash is used to determine if the source query has changed and trigger re-submitted it.  Otherwise, the downloader will simply grab the output from the <code>job</code> specified.</p>"},{"location":"research/building/#cleanup","title":"Cleanup","text":"<p>There are several <code>make</code> targets to clean up:</p> <ul> <li><code>make clean</code> cleans up some temporary files and analysis outputs</li> <li><code>make clean-figures</code> removes generated figures (<code>.pdf</code> and .png anywhere in <code>figures/</code>)</li> <li><code>make clean-tables</code> removes generated tables (<code>.tex</code> anywhere in <code>tables/</code>)</li> <li><code>make clean-txt</code> removes downloaded TXT files</li> <li><code>make clean-csv</code> removes generated CSV files</li> <li><code>make clean-pq</code> removes cached/intermediate Parquet files</li> <li><code>make clean-zip</code> removes generated ZIP files</li> <li><code>make clean-all</code> runs all the clean targets</li> </ul>"},{"location":"research/common/","title":"Using the Common Library","text":"<p>The study template provides a common Python library to help researchers generate tables and figures from their analyses.</p>"},{"location":"research/common/#data-management","title":"Data Management","text":"<p>The <code>common.df</code> library provides two functions for reading Boa output into a Pandas dataframe, these are <code>get_df</code> and <code>get_deduped_df</code>.  These functions take similar basic arguments.  These functions will read from a Parquet file if it has been generated, otherwise, they will read from CSV (and save to parquet for sped-up loading later on).  Basic call syntax is below, with description of arguments following.</p> Helpers for reading Boa output into Pandas dataframes<pre><code># read all data\nget_df(filename: str, subdir: Optional[str]=None,\n       drop: Optional[List[str]]=None,\n       precache_function: Optional[Callable[[pd.DataFrame], pd.DataFrame]]=None,\n       **kwargs) -&gt; pd.DataFrame\n\n# read de-duplicated data\nget_deduped_df(filename: str, subdir: Optional[str]=None, dupesdir: Optional[str]=None,\n               drop: Optional[List[str]]=None,\n               precache_function: Optional[Callable[[pd.DataFrame], pd.DataFrame]]=None,\n               ts: bool=False, **kwargs) -&gt; pd.DataFrame\n</code></pre> <ul> <li><code>filename</code>: the name of the CSV data file, without <code>.csv</code>.</li> <li><code>subdir</code>: optional, the name of the sub-directory underneath <code>data/csv/</code> that <code>filename</code> is in (default <code>None</code>).</li> <li><code>dupesdir</code>: (<code>get_deduped_df</code> only): optional, the name of the sub-directory underneath <code>data/csv</code> containing the dupes file (default <code>None</code>).</li> <li><code>drop</code>: A list of column names to drop after loading.</li> <li><code>precache_function</code>: A function that takes a data frame, and transforms it in some way (e.g., creating new columns which are intensive to compute, or converting data types).</li> <li><code>ts</code> (<code>get_deduped_df</code> only): Pass <code>True</code> if the hash file also has file timestamps.</li> <li><code>**kwargs</code>: When reading from CSV, these are passed to <code>pd.read_csv</code>.</li> </ul> <p>An example of the usage of <code>get_deduped_df</code> is shown below.</p> analyses/rq1.py<pre><code>    df = get_deduped_df('rq1', 'kotlin', 'kotlin', names=['var', 'project', 'file', 'astcount'])\n</code></pre> <p>This will get a file-wise deduplicated dataframe from the results file <code>rq1.csv</code> in <code>data/csv/kotlin/</code>, using the <code>data/csv/kotlin/dupes.csv</code> file to provide duplication information.  It gives the columns the names <code>var</code>, <code>project</code>, <code>file</code>, and <code>astcount</code>.</p>"},{"location":"research/common/#deduplication","title":"Deduplication","text":"<p>Since data duplication is a known problem in MSR studies (see Lopes et al., 2017), we provide the ability to deduplicate data.  However, this deduplication is based on AST hashes.  This is done by calculating the hash of the AST of each file as it appears in the HEAD commit of each repository, and selecting one project/file pair for each hash value.  A query for this is provided, see also Defining Queries.</p>"},{"location":"research/common/#table-generation","title":"Table Generation","text":"<p>Pandas can be used to generate LaTeX tables from query output and calculated data, however, much of this is routine and enabled by <code>common.tables</code>.  In particular, <code>common.tables</code> generates tables that use the <code>booktabs</code> package for formatting (following the ACM document class recommendations).</p> <p>To do this, there are four major functions:</p> <ul> <li><code>get_styler</code> which returns a <code>Styler</code> object for a dataframe or series.  Stylers are used to format data based on the values of each cell.  In addition to the dataframe or series, it takes two keyword arguments: a number of <code>decimals</code> (default 2), and a <code>thousands</code> separator (default <code>,</code>).</li> <li><code>highlight_cols</code> and <code>highlight_rows</code>: These highlight the column and row headers, respectively of a table in a <code>Styler</code> object, as shown below on line 11.</li> <li><code>save_table</code> will save a <code>Styler</code> to a LaTeX table.  Its usage is somewhat more complex, and is described below.</li> </ul> analyses/rq1.py<pre><code>    style = highlight_rows(highlight_cols(get_styler(df)))\n    save_table(style, 'rq1.tex', 'kotlin')\n</code></pre>"},{"location":"research/common/#using-save_table","title":"Using <code>save_table</code>","text":"The save_table() function<pre><code>def save_table(styler: pandas.io.formats.style.Styler, filename: str,\n               subdir: Optional[str]=None,\n               mids: Optional[Union[RuleSpecifier, List[RuleSpecifier]]]=None,\n               colsep: Optional[str]=None, **kwargs):\n</code></pre> <p><code>save_table</code> takes two mandatory arguments, a <code>styler</code>, and a <code>filename</code> (which should include the <code>.tex</code> extension).  It takes an optional <code>subdir</code> (underneath <code>tables/</code>) to save the file in as well.  Additionally, the keyword argument <code>colsep</code> is available to use a custom column separator width, if no argument (or <code>None</code>) is passed, defaults will be used, otherwise, the value should be the size of the column separator in LaTeX compatible units.</p> <p>Additionally, a <code>mids</code> keyword argument is available to allow manual placement of mid-table rules.  If <code>None</code>, no mid-table rules will be passed, otherwise, a rule specifier or a list of rule specifiers may be passed, as described below.</p> <p><code>RuleSpecifier</code>s take the following form:</p> <ul> <li>A single integer \\(n\\), which will place a <code>\\midrule</code> after the \\(n\\)th line (one-based indexing).</li> <li>A pair <code>(n, width)</code> will place <code>\\midrule[width]</code> after the \\(n\\)th line.</li> <li>A pair <code>(n, cmidrulespec)</code> or <code>(n, [cmidrulespec+])</code>, which will place the specified <code>cmidrules</code> after the \\(n\\)th line.  A <code>cmidrulespec</code> is a tuple, <code>(start, end, left_trim, right_trim)</code>, where <code>start</code> and <code>end</code> are column indices, and <code>left_trim</code> and <code>right_trim</code> are either Booleans or LaTeX lengths.  If they are False, no trim will be applied, if they are True, default trim will be applied, if they are a LaTeX length, a trim of that length will be applied.</li> </ul> <p>Finally, additional keyword arguments may be passed to <code>styler.to_latex</code>, to further control generated appearance.  Options of note include <code>multirow_align</code> to control the vertical alignment of row-spanning cells, <code>multicol_align</code> to control the horizontal alignment of column-spanning cells, and <code>siunitx</code> to enable <code>siunitx</code>-style numerical alignment.</p>"},{"location":"research/common/#figure-generation","title":"Figure Generation","text":"<p>The <code>df.graphs</code> module provides a function, <code>setup_plots</code> to create a blank, pre-configured plot canvas for use.  It takes an optional argument, <code>rcParams</code>, which is used to set the <code>plt.rcParams</code> parameters.  In particular, the following are set by default:</p> <ul> <li>PDF and PS font types are set to 42, avoiding PostScript Type 3 fonts (for compliance with common submission requirements).</li> <li>Figure size is set to 6\"x4\", with 600 DPI.</li> <li>Font size is set to 24 pt.</li> <li>Plots are set in a constrained layout (see Matplotlib's constrained layout guide for more information).</li> </ul>"},{"location":"research/common/#utilities","title":"Utilities","text":"<p>Finally, a few utilities are provided in <code>common.utils</code>.  These are mostly intended for helping to simplify analyses, and are as follows:</p> <ul> <li><code>get_dataset</code> will take a filename base name and optional sub-directory name, and determine which Boa dataset the data came from.</li> </ul>"},{"location":"research/common/#loading-common-libraries","title":"Loading Common Libraries","text":"<p>The common libraries described above can be loaded as normal in most cases.  However, if analyses are arranged in various subdirectories, the following code can be used to allow import.</p> Code to import common from a subdirectory of 'analyses/'.<pre><code>from pathlib import Path\nimport sys\nsys.path.append(str(Path(__file__).resolve().parent.parent)) # (additional calls to parent may be necessary for deeply-nested analyses)\n</code></pre>"},{"location":"research/paths/","title":"Paths","text":"<p>Here is an overview of the folder layout of the study template.</p> <p>There are a few files in the root to be aware of.  There is a README and LICENSE file for the study template.  Note that the README is named <code>README-study-template.md</code>.  There is a second README named <code>sample-README.md</code> that is a template for the README for your study.  You should copy this file to <code>README.md</code> and edit it to describe your study.  Also update the <code>LICENSE</code> file to reflect the license for your study.</p> <p>There is also a <code>Makefile</code> to control running the analyses.  To go with that, a <code>Makefile.study</code> gets automatically regenerated as you modify your study.</p> <p>The <code>study-config.json</code> is the main entry point for the study.  This is where you define datasets to query, queries, where to store the output, and the analyses to run on that data.  This is the file that you will need to edit. This is described further in other sections of the documentation.</p> <p>Finally, there is a <code>jobs.json</code> that tracks the Boa job numbers for any queries run. This file should not be directly modified.</p>"},{"location":"research/paths/#queries-boa","title":"Queries: <code>boa/</code>","text":"<p>These are the Boa queries used to generate data for the paper.  You are free to organize this folder however you like, for example by making subfolders for different categories of queries.  If your study is querying multiple languages, you may want to make a subfolder for each language.</p>"},{"location":"research/paths/#data-data","title":"Data: <code>data/</code>","text":"<p>This is the output of the Boa queries (the <code>.txt</code>) files, as well as processed versions of that output (<code>.csv</code> and <code>.parquet</code>).  While it can be sometimes useful to inspect the raw output, most of the time you should not need to access files in this folder.</p>"},{"location":"research/paths/#analyses-analyses","title":"Analyses: <code>analyses/</code>","text":"<p>This is where the Python scripts for generating figures and tables go.</p> <p>There is a subfolder <code>common/</code> that contains helper functions for processing the data and generating figures/tables.</p>"},{"location":"research/paths/#generated-figures-figures","title":"Generated Figures: <code>figures/</code>","text":"<p>Any generated figures (<code>.pdf</code>) from the analyses will go into this folder.</p>"},{"location":"research/paths/#generated-tables-tables","title":"Generated Tables: <code>tables/</code>","text":"<p>Any generated tables (<code>.tex</code>) from the analyses will go into this folder.</p>"},{"location":"research/paths/#other-folders","title":"Other Folders","text":"<p>The <code>bin/</code> folder has scripts used by the study template for things like submitting queries to Boa and downloading output, managing the Zenodo deposit, and keeping the Makefile up to date.  You generally will not need to manually run most of these, as the Makefile should manage things for you.</p>"},{"location":"research/publishing/","title":"Publishing","text":""},{"location":"research/publishing/#adding-a-readmemd-file","title":"Adding a README.md file","text":"<p>Your replication package will need a README file.  We provide a template file, <code>sample-README.md</code>, that you can use as a starting point for your package.  Just rename the file to <code>README.md</code> and edit accordingly. There are several places with \"TODO\" notices that you will definitely want to change.  And we recommend also adding a section at the end that describes each analysis in a bit more detail.</p>"},{"location":"research/publishing/#optional-configuring-docker-support","title":"(optional) Configuring Docker Support","text":"<p>The study template has a sample <code>Dockerfile</code> to build a Python-based container that is capable of building the study.  You will need to update two things. First, update the version of Python used.  Currently, the Dockerfile is set to Python 3.10, so if you used a newer version of Python be sure to update.  Try to use a concrete version (e.g., <code>3.11.2</code>) rather than a generic version. Second, you will want to edit the <code>Makefile</code> to change the name of the generated Docker image.  By default, the image will be named \"study-template\".</p>"},{"location":"research/publishing/#packaging","title":"Packaging","text":"<p>If you run <code>make package</code>, replication package ZIP files will be generated.  It generates three different ZIP files:</p> <ul> <li><code>replication-pkg.zip</code><ul> <li>The main replication package and should include all Boa queries, analysis scripts, support scripts, and additional things like the documentation and Makefile.</li> </ul> </li> <li><code>data-cache.zip</code><ul> <li>The pre-processed Parquet files.  This is the cache of the raw data files, in a compressed binary format.</li> </ul> </li> <li><code>data.zip</code><ul> <li>The contents of the <code>data/txt/</code> folder, which is the raw outputs from Boa.</li> </ul> </li> </ul> <p>Most people simply trying to regenerate the tables/figures will only need the first and second files.  The file with the full data is typically quite large and only needed if someone plans to extend/enhance the analyses somehow.</p> <p>Note that the command updates existing ZIP files if they exist, so you can simply re-run it after small changes to a few files and it will update.  It will not, however, remove any files from the ZIP files so if you wind up deleting a query/analysis you either need to remove the ZIPs with <code>make clean-zip</code> and regenerate, or manually run the <code>zip</code> command and remove file(s) from the generates ZIPs.</p>"},{"location":"research/publishing/#publishing-the-package","title":"Publishing the Package","text":"<p>Once you have build your replication package, you can also upload it to Zenodo. To do so, you will need to create a <code>.env</code> file with the following contents:</p> .env<pre><code>ZENODO_API_TOKEN='&lt;your API token here&gt;'\nZENODO_API_ENDPOINT='https://zenodo.org'\n# or for testing:\nZENODO_API_ENDPOINT='https://sandbox.zenodo.org'\n</code></pre> <p>This requires logging into Zenodo and creating an API token.  Be sure to not share the <code>.env</code> file with anyone once you create it!  It will not be placed into any ZIP files and is ignored by Git, but you will still want to be careful with it.</p> <p>Then select an API endpoint.  If you want to test creating a Zenodo record you can utilize their sandbox server.  Otherwise, use the main server.  Note that if you use the sandbox, you need to create a token on that server, as tokens are not shared across the two servers.</p> <p>To utilize the script, simply run <code>make zenodo</code>.</p> <p>The metadata for the Zenodo record is stored in the <code>.zenodo.json</code> file.  This file can be shared and by default is stored in Git.  If one does not exist, the first time you run the command one will be generated and it will stop processing to allow you time to edit it. This file contains the metadata for your record, including things like the title, description, creators, and license info.  By default, we selected CC-By-4.0 as the license, so feel free to change it if needed.</p> <p>For a double-blinded submission, you will want to ensure the creators are listed as anonymous and the access rights set to \"open\":</p> .zenodo.json<pre><code>    \"creators\": [\n        {\n            \"affiliation\": \"Anonymous\",\n            \"name\": \"Anonymous\"\n        }\n    ],\n    \"access_right\": \"open\",\n</code></pre> <p>After your paper is published, you can update the metadata and re-run the script to have it update the metadata with your actual author name(s).</p> <p>For more details on the metadata JSON format, see this link: https://developers.zenodo.org/#representation</p>"},{"location":"research/templates/","title":"Query Templates","text":"study-config.json<pre><code>  \"substitutions\": [\n    {\n      \"target\": \"{@escape@}\",\n      \"file\": \"escape.boa\"\n    },\n    {\n      \"target\": \"{@project-filter@}\",\n      \"replacement\": \"if (input.stars &gt;= 5)\"\n    }\n  ],\n</code></pre> <p>Query templates are defined through a list of substitutions.  Substitutions are defined using two keys.  The <code>target</code> key is mandatory, containing the text which is replaced in the query.  Targets must start with <code>{@</code> and end with <code>@}</code>.  Target names are restricted to alphanumeric characters, and some special characters like <code>-_:.</code>.  Then, either <code>replacement</code> or <code>file</code> are used, with <code>replacement</code> providing the text directly, and <code>file</code> being the name of a file under <code>boa/snippets/</code> which replaces <code>target</code>.  Care must be taken to ensure that the query is valid Boa code after substitution is completed.</p> <p>Before performing substitutions, a substitutions list is constructed, first from local substitutions then from global substitutions.  If two substitutions define the same <code>target</code>, the first one defined is used.  Substitution will iterate through the substitutions list until a loop has been completed without any substitutions (i.e., a steady state has been reached).</p>"}]}