{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Boa Study Template","text":"<p>Boa is a domain-specific language and infrastructure that eases mining software repositories. Boa's infrastructure leverages distributed computing techniques to execute queries against hundreds of thousands of software projects very efficiently.</p> <p>Boa's Study Template was designed to aid researchers who want to use Boa in a research study.  The goal was two-fold: first, to make it easier to write, run, and process Boa queries and outputs, and second, to enable better replication of research.</p> <p>To accomplish these goals, the study template automatically manages a lot of the details such as when to run a Boa query, when to download its results, and automatic conversion of the output to CSV.  Additionally, the study template provides some helper functions in Python to make it easier to analyze that data.  Generally, researchers using the study template simply need to write their Boa queries, write the Python analyses to process the output, and then run <code>make</code> to execute the study.  The study template figures out what queries need to be run, what outputs need to be downloaded, and what analyses need to be run based on what files have changed since the last run.</p> <p>Moreover, the study template has reproducibility in mind.  It automatically records information about the Boa jobs that were run and stores the data in a converted, compressed Parquet file.  It also provides <code>make</code> targets to produce replication package ZIP files that contain all the data and analyses needed to reproduce the study.  This includes the Boa queries, the Python analyses, the data, and the generated figures and tables.</p> <p>The study template also provides support for publishing your replication package on Zenodo.  This is a great way to get a DOI for your replication package and to make it easy for others to cite your work.  The study template will automatically generate a Zenodo metadata file and upload the replication package to Zenodo.  By default, it will support double-blinded submissions and provides an easy way to unblind your replication after your paper is accepted.</p>"},{"location":"installing/","title":"Installing","text":"<p>Read below for instructions on installing the Boa Study Template.  Note that the instructions will differ a bit depending on whether you are starting a new study or using a replication package.</p>"},{"location":"installing/#boa-api-credentials","title":"Boa API Credentials","text":"<p>Boa API Access for New Studies</p> <p>If you are starting a new research study and utilizing Boa's study template, a username/password to the Boa website and API are required.  See the full list of requirements for more details.</p> <p>Boa API Access for Replication Packages</p> <p>If you are using a replication package that utilized Boa's study template, you should not need any Boa API credentials (unless you wish to re-run the Boa queries in the package or intend to make changes to the study).</p> <p>Once you have a Boa API login, edit the file <code>.env</code> and enter your username as <code>BOA_API_USER='&lt;username here&gt;'</code>, then save and close the file.</p> <p>We recommend also setting your password in your OS keyring, so you are not prompted to enter it each time a download triggers.  To do this, first install the <code>keyring</code> Python package and then run:</p> <pre><code>keyring set boaapi &lt;username&gt;\n</code></pre> <p>where <code>&lt;username&gt;</code> is your Boa username.</p> <p>If you are unable to use the keyring, you can also enter your password into the <code>.env</code> file as <code>BOA_API_PW='&lt;password here&gt;'</code>.</p> <p>Finally, if the script can't find your username and/or password, it will prompt you for it.  Note however it will prompt for each file it downloads.</p>"},{"location":"installing/#using-a-replication-package","title":"Using a Replication Package","text":"<p>Most likely, the only thing you need to do to use a replication package is to unzip the <code>replication-pkg.zip</code> file (or equivalent).  This will create a directory with the Boa queries, the Python analyses, and the generated figures and tables.</p> <p>If you also wish to re-run the analyses, you will need the <code>data-cache.zip</code> file (or equivalent).  Unzip this file starting in the root of the replication package and it will place files into the <code>data/parquet/</code> folder.</p> <p>If you wish to generate new analyses, you may need the raw data.  You will need the <code>data.zip</code> file (or equivalent).  Unzip this file starting in the root of the replication package and it will place files into the <code>data/txt/</code> folder.</p>"},{"location":"installing/#starting-a-new-study","title":"Starting a new Study","text":"<p>To start a new study, first clone the study template from GitHub:</p> <pre><code>git clone https://github.com/boalang/study-template.git\n</code></pre> <p>Then follow our guide on using the template in a research study.</p>"},{"location":"replications/","title":"Using a Replication","text":"<p>This replication package utilizes the Boa Study Template.  If you want to work with this package beyond simply re-generating the paper's figures and tables, we strongly encourage you to utilize Visual Studio Code with the Boa Language and Infrastructure Extension installed.  For more information, see Using the Template.</p> <p>If you have that extension, you can avoid running terminal commands (like <code>make</code>) and instead simply open the study-config.json file to control what files you download, which analyses you run, etc.  For more details on working with Boa's study template, see the online documentation.</p>"},{"location":"replications/#requirements","title":"Requirements","text":"<p>You need a GNU Make compatible build system.  Tested on GNU Make 3.81.</p> <p>Note that the Boa jobs themselves are marked public, so you do not need a Boa user to view the actual jobs/output via the website.  However, the Boa API requires a user/password to use it, so programmatically downloading (even public jobs) currently requires authenticating.  You can, however, manually download each of the query outputs from the public URLs.</p>"},{"location":"replications/#python-requirements","title":"Python Requirements","text":"<p>Python 3 is required for all scripts.  Tested on Python TODO: put Python version used here.</p> <p>See <code>requirements.txt</code> for the package requirements.  To install them, run:</p> <pre><code>pip3 install -r requirements.txt\n</code></pre> <p>There are also optional packages you can install to improve your experience.</p>"},{"location":"replications/#optional-python-requirements","title":"Optional Python Requirements","text":"<p>If you install <code>tqdm&gt;=4.64.0</code>, some scripts will display a progress bar.  This can be very useful when processing extremely large (multi-GB) output files.</p> <p>If you install <code>keyring&gt;=23.8.2</code>, you can store your Boa API credentials in your OS's keyring.  This is more secure than storing it in plaintext in the <code>.env</code> file.  If you don't utilize either of those options, it will prompt you for your username/password (once for each output it tries to download!).</p>"},{"location":"replications/#docker-support","title":"Docker Support","text":"<p>If you have Docker installed, you can use the provided <code>Dockerfile</code> to build an image capable of running all the scripts.  This is the easiest way to get a working environment.  To build and run the image, run:</p> <pre><code>make run-docker\n</code></pre>"},{"location":"replications/#file-organization","title":"File Organization","text":"<p>Here is an overview of the folders layout.  The root contains scripts for generating tables/figures for the paper, a <code>Makefile</code> for running scripts, and this <code>README.md</code> file.</p>"},{"location":"replications/#subdir-boa","title":"Subdir: <code>boa</code>","text":"<p>These are the Boa queries used to generate data for the paper.</p>"},{"location":"replications/#subdir-data","title":"Subdir: <code>data</code>","text":"<p>This is the output of the Boa queries (the <code>.txt</code>) files, as well as processed versions of that output (<code>.csv</code> and <code>.parquet</code>).</p>"},{"location":"replications/#subdir-figures","title":"Subdir: <code>figures</code>","text":"<p>Any generated figures (<code>.pdf</code>) will go into this folder.</p>"},{"location":"replications/#subdir-tables","title":"Subdir: <code>tables</code>","text":"<p>Any generated tables (<code>.tex</code>) will go into this folder.</p>"},{"location":"replications/#getting-boa-output","title":"Getting Boa Output","text":"<p>NOTE: This step is only needed if you don't already have the output downloaded!  If you downloaded <code>data.zip</code>, you can skip this step!</p> <p>The first step is to run Boa queries to generate output TXT data for further processing: <code>make txt</code></p>"},{"location":"replications/#processing-the-boa-output","title":"Processing the Boa Output","text":"<p>The Boa output is in a custom format, so first we convert it all into standard CSV format: <code>make csvs</code></p> <p>If you use the <code>make data</code> command instead of manually obtaining the outputs, you do not need to do anything else as it will call this target for you.</p>"},{"location":"replications/#generating-figures-and-tables","title":"Generating Figures and Tables","text":"<p>To generate all the figures and tables for the paper, you need to run the analysis for each specific research question on the output from Boa.  There is also a helper target to run all analyses:</p> <pre><code>make analysis\n</code></pre> <p>Note that this triggers download of any missing Boa query outputs.  Or you can run all analyses on only the cached data:</p> <pre><code>make reproduce\n</code></pre> <p>If you want to run individual analyses, you can also do so:</p> <pre><code>make rq1**TODO: update for your analysis script names**\n</code></pre> <pre><code>make rq2\n</code></pre> <p>You can also run a single analysis on the cached data by adding <code>-reproduce</code> to the target name:</p> <pre><code>make rq1-reproduce**TODO: update for your analysis script names**\n</code></pre> <pre><code>make rq2-reproduce\n</code></pre>"},{"location":"requirements/","title":"Requirements","text":"<p>You need a GNU Make compatible build system.  Tested on GNU Make 3.81.</p>"},{"location":"requirements/#boa-api-access","title":"Boa API Access","text":"<p>A username/password to the Boa website and API are required to use the study template.  You can request one here: https://boa.cs.iastate.edu/request/</p>"},{"location":"requirements/#python-requirements","title":"Python Requirements","text":"<p>Python 3 is required for all scripts.  Tested on Python 3.9.12.</p> <p>See <code>requirements.txt</code> for the package requirements.  To install them, run:</p> <pre><code>pip3 install -r requirements.txt\n</code></pre> <p>There are also optional packages you can install to improve the experience.</p>"},{"location":"requirements/#optional-python-requirements","title":"Optional Python Requirements","text":"<p>If you install <code>tqdm</code>, some scripts will display a progress bar.  This can be very useful when processing extremely large (multi-GB) output files.</p> <p>If you install <code>keyring</code>, you can store your API credentials in your OS's keyring.  This is more secure than storing it in plaintext in the <code>.env</code> file.  If you don't utilize either of those options, it will prompt you for your username/password (once for each output it tries to download!).</p> <p>See <code>requirements-optional.txt</code> for the package requirements.  To install them, run:</p> <pre><code>pip3 install -r requirements-optional.txt\n</code></pre>"},{"location":"vscode/","title":"VSCode Extension","text":"<p>Boa provides a Visual Studio Code extension on the marketplace and the Open VSX Registry.</p> <p>Boa API Access</p> <p>While you can use this extension to view and write Boa queries, most functionality (like submitting and managing jobs) requires having a user/password on the Boa infrastructure. If you do not yet have one, please request a user.</p>"},{"location":"vscode/#features","title":"Features","text":""},{"location":"vscode/#viewing-and-writing-boa-queries","title":"Viewing and Writing Boa Queries","text":"<p>This extension provides support for viewing and writing Boa queries:</p> <p></p> <p>This includes things like syntax highlighting, the ability to insert useful code snippets, code completion, and support for study template substitutions (templates).</p> <p>Any substitutions in the <code>study-config.json</code> are automatically highlighted by the editor.  If you hover over one, you can see the value it will be replaced with.  The code completion also includes these substitutions.</p> <p>Note that since the editor does not know the context of which query you are editing, it will show all possible substitutions that could match.  If you want to view the file for a particular query, you can click the preview button in the top right of the editor and select the query you want to preview it for.</p>"},{"location":"vscode/#submitting-and-managing-boa-jobs","title":"Submitting and Managing Boa Jobs","text":"<p>The extensions also provides support for submitting queries to Boa and managing existing jobs:</p> <p></p> <p>This allows you to effectively stay in the IDE and never need to go to the Boa website.</p>"},{"location":"vscode/#using-boas-study-template","title":"Using Boa's Study Template","text":"<p>If you are using the study template, either for a new study or a replication package, the extension provides additional support for managing the study:</p> <p></p> <p>Simply open the <code>study-config.json</code> file and you will see several code actions available to you that allow you to download specific outputs, clean data, run specific analyses, etc.  This is an alternative to running <code>make</code> on the command line.  There is also a tree view that shows the various outputs and analyses that can be run and allows clicking on it to run them.</p> <p>Most of the filenames shown in the file can also be clicked on to open the file in the editor.</p>"},{"location":"vscode/#extension-settings","title":"Extension Settings","text":"<p>The extension will automatically prompt you for your Boa API credentials the first time you use it.</p> <p>This extension also contributes the following settings:</p> <ul> <li><code>boalang.dataset.favorite</code>: Favorite Boa dataset</li> <li><code>boalang.joblist.pagesize</code>: Number of jobs to show in the jobs tree view</li> <li><code>boalang.output.size</code>: Size (in bytes) to limit displaying query outputs</li> <li><code>boalang.joblist.autoload</code>: Should the jobs tree view refresh on extension activation?</li> </ul>"},{"location":"research/","title":"Using the Template","text":"<p>If you downloaded a replication package that utilizes the study template, please see the section on Using a Replication.</p> <p>This section of the documentation is intended for researchers using the study template to perform initial research.</p>"},{"location":"research/add-analysis/","title":"Adding Analyses","text":"<p>There is one sample analysis given in <code>analyses/rq1.py</code>. This relies on the sample query given in <code>boa/queries/rq1.boa</code> and makes use of de-duplication, which relies (indirectly) on the query given in <code>boa/queries/hashes.boa</code>.  This analysis will generate a single result, the table in <code>tables/kotlin/rq1.tex</code>.</p> <p>The steps to add a new analysis are as follows:</p> <ol> <li>Create a new Boa query (e.g., <code>foo.boa</code>):<ol> <li>Store the file in <code>boa/queries/</code> - note you can create sub-folders here if you want.</li> <li>Add any missing common snippets into <code>boa/snippets/</code>.</li> <li>Add the query into the <code>study-config.json</code>.</li> <li>Repeat Step 1 as many times as necessary.</li> </ol> </li> <li>Create a new Python script to analyze the data (e.g., <code>foo.py</code>) in the top folder.</li> <li>Add a new entry to the <code>study-config.json</code> in the <code>analyses</code> object.<ol> <li>The name of the entry is the script filename (e.g., <code>foo.py</code>, without the <code>analyses/</code> prefix).</li> <li>Add an <code>input</code> key, that is an array of CSV filenames that the analysis depends on.</li> </ol> </li> </ol> <p>Once this is done, you should be able to run <code>make foo</code> (the target is the name of the script, without the file extension) to run the analysis task, or run <code>make analysis</code> to run all analysis tasks.</p>"},{"location":"research/add-query/","title":"Adding Queries","text":"<p>All information about what queries to run and where to download output is stored in the <code>study-config.json</code> file.  This file is used to automatically create a <code>Makefile.study</code> with make targets for each output file.</p> <p>All Boa queries must reside under the <code>boa/</code> folder.  There are two sub-folders under the query folder: <code>boa/queries/</code> and <code>boa/snippets/</code>.  <code>boa/queries/</code> is where you store most of your Boa queries.  A few re-usable queries and examples are already provided there.</p> <p>The <code>boa/snippets/</code> folder is where the query template system looks for any included files.  The template system is described in the next section.</p>"},{"location":"research/add-query/#study-configjson-schema","title":"study-config.json Schema","text":"<p>The most important file for your study is the <code>study-config.json</code> configuration file.  This file defines all datasets, all queries (and their output file paths), and any template substitutions needed to build the queries.</p>"},{"location":"research/add-query/#defining-datasets","title":"Defining Datasets","text":"study-config.json<pre><code>  \"datasets\": {\n    \"kotlin\": \"2021 Aug/Kotlin\",\n    \"python\": \"2021 Aug/Python\",\n    \"python-ds\": \"2020 August/Python-DS\",\n    \"java\": \"2019 October/GitHub\",\n    \"java-sf\": \"2013 September/SF\",\n    \"original\": \"2012 July/SF\"\n  },\n</code></pre> <p>The first thing to include is at least one dataset.  Datasets are listed in the <code>datasets</code> object.  Each dataset must have a unique <code>custom_name</code>.  This is the name used to reference the dataset in a query.  The value is the name of the Boa dataset, as on the Boa website.  You can list as many datasets here as you want.</p> <p>Listing datasets here makes it easy to repeat a study on a new dataset, as you only need to update a single entry in the JSON file and all queries will pick up the change.</p>"},{"location":"research/add-query/#defining-queries","title":"Defining Queries","text":"<p>The next object in the study config is the <code>queries</code> object:</p> study-config.json<pre><code>  \"queries\": {\n    \"kotlin/hashes.txt\": {\n      \"query\": \"queries/hashes.boa\",\n      \"dataset\": \"kotlin\",\n      \"processors\": {\n        \"gendupes.py\": {\n          \"output\": \"data/txt/kotlin/dupes.txt\",\n          \"csv\": \"kotlin/dupes.csv\",\n          \"cacheclean\": [\n            \"kotlin/*-deduped.parquet\"\n          ]\n        }\n      }\n    },\n    \"kotlin/rq1.txt\": {\n      \"query\": \"queries/rq1.boa\",\n      \"dataset\": \"kotlin\",\n      \"csv\": {\n        \"output\": \"kotlin/rq1.csv\",\n        \"test\": [\n          \"3,\\\\.kts?$\"\n        ]\n      }\n    },\n    \"kotlin/project-count.txt\": {\n      \"query\": \"queries/project-count.boa\",\n      \"dataset\": \"kotlin\",\n      \"csv\": {\n        \"output\": \"kotlin/project-count.csv\",\n        \"drop\": [\n          1\n        ]\n      }\n    },\n    \"python/project-count.txt\": {\n      \"query\": \"queries/project-count.boa\",\n      \"dataset\": \"python\",\n      \"csv\": {\n        \"output\": \"python/project-count.csv\",\n        \"drop\": [\n          1\n        ]\n      }\n    }\n  },\n</code></pre> <p>This object contains output filenames as keys.  Note that all output here is coming from Boa queries, so they are always TXT files stored under <code>data/txt/</code>.  You omit that prefix in the file paths listed here. You can include sub-folders if you want.</p> <p>The values must have, at a minimum, <code>query</code> and <code>dataset</code> keys listing the path to the Boa query stored under <code>boa/</code> (again, you omit that prefix here, but you can point to any path under that folder) and the name of a dataset defined earlier.</p> <p>If for some reason you don't want the Boa job to be marked public, you can set the <code>public</code> key to <code>false</code>.  By default, all submitted jobs will be marked public after submission.</p> <p>A query can also indicate if it should be converted to CSV format.  Most queries probably want to convert to CSV, so you can easily load the data into Pandas for analysis.  This is indicated by adding a <code>csv</code> key.  The value is either a string listing the output path for the CSV file (stored in <code>data/csv/</code>, with the prefix omitted here) or an object listing the <code>output</code> path and some optional parameters:</p> <ul> <li><code>test</code> (can be repeated)</li> <li>Add a <code>\"column,test\"</code> pair, where the given column keeps consuming the row     until the given regex test matches. This is useful because the output from     Boa does not escape, so if a column (other than the last) contains strings     and if those strings wind up having <code>][</code> in them (as some filenames do),     the conversion script might break and create a jagged CSV table.</li> <li><code>drop</code> (can be repeated)</li> <li>Drop a column (0-indexed) when converting.</li> <li><code>header</code></li> <li>A header row to prepend to the CSV output.  Can be useful if you think     others might use the generated CSV files outside your own analyses.</li> <li><code>index</code></li> <li>Number of indices in the Boa output - if not given, infers from the first     line.  This is usually not needed.</li> </ul> <p>Finally, a query can also indicate if the <code>gendupes.py</code> script should run on the output file.  This is used for queries that output file hashes from Boa, to allow identifying duplicate files (based on matching AST hashes) for later de-duplication during analysis.  The value takes the <code>output</code> path where to store the generated TXT file with duplicate hash data (here, you must provide the <code>data/txt/</code> prefix).  It can also provide an optional <code>csv</code> key to convert the generated TXT file into CSV format (with the prefix omitted).  An optional <code>cacheclean</code> key allows listing additional cache (Parquet) files to clean up when re-generating this output.</p>"},{"location":"research/add-query/#defining-substitutions","title":"Defining Substitutions","text":"study-config.json<pre><code>  \"substitutions\": [\n    {\n      \"target\": \"{@escape@}\",\n      \"file\": \"escape.boa\"\n    },\n    {\n      \"target\": \"{@project-filter@}\",\n      \"replacement\": \"if (input.stars &gt;= 5)\"\n    }\n  ],\n</code></pre> <p>The next object is the list of global template substitutions.  These substitutions are available to every query, but can be overridden by specific queries.  See templates for more details.</p>"},{"location":"research/add-query/#defining-analyses","title":"Defining Analyses","text":"study-config.json<pre><code>  \"analyses\": {\n    \"rq1.py\": {\n      \"input\": [\n        \"kotlin/rq1.csv\",\n        \"kotlin/dupes.csv\"\n      ]\n    }\n  }\n</code></pre> <p>The last object is the list of analysis scripts you want to run.  See the section on adding an analysis for more details.</p>"},{"location":"research/building/","title":"Building","text":"<p>To build, run <code>make</code>.  This will submit all queries, download their outputs, convert them (where necessary), and run all analysis tasks to generate the output tables/figures.</p> <p>As Boa queries are submitted, they are marked public (unless specified not to) and details about the submitted Boa job are cached in <code>jobs.json</code>. This file contains keys that are the name of an output TXT file (without the <code>data/txt/</code> prefix).  The values are the <code>job</code> number and a <code>sha256</code> hash of the source query (after running through the template engine). The hash is used to determine if the source query has changed and trigger re-submitted it.  Otherwise, the downloader will simply grab the output from the <code>job</code> specified.</p>"},{"location":"research/building/#cleanup","title":"Cleanup","text":"<p>There are several <code>make</code> targets to clean up:</p> <ul> <li><code>make clean</code> cleans up some temporary files and analysis outputs</li> <li><code>make clean-figures</code> removes generated figures (<code>.pdf</code> and .png anywhere in <code>figures/</code>)</li> <li><code>make clean-tables</code> removes generated tables (<code>.tex</code> anywhere in <code>tables/</code>)</li> <li><code>make clean-txt</code> removes downloaded TXT files</li> <li><code>make clean-csv</code> removes generated CSV files</li> <li><code>make clean-pq</code> removes cached/intermediate Parquet files</li> <li><code>make clean-zip</code> removes generated ZIP files</li> <li><code>make clean-all</code> runs all the clean targets</li> </ul>"},{"location":"research/paths/","title":"Paths","text":"<p>Here is an overview of the folders layout.  The root contains scripts for generating tables/figures for the paper, a <code>Makefile</code> for running scripts, and a <code>README.md</code> file.</p>"},{"location":"research/paths/#subdir-boa","title":"Subdir: boa","text":"<p>These are the Boa queries used to generate data for the paper.</p>"},{"location":"research/paths/#subdir-data","title":"Subdir: data","text":"<p>This is the output of the Boa queries (the <code>.txt</code>) files, as well as processed versions of that output (<code>.csv</code> and <code>.parquet</code>).</p>"},{"location":"research/paths/#subdir-figures","title":"Subdir: figures","text":"<p>Any generated figures (<code>.pdf</code>) will go into this folder.</p>"},{"location":"research/paths/#subdir-tables","title":"Subdir: tables","text":"<p>Any generated tables (<code>.tex</code>) will go into this folder.</p>"},{"location":"research/publishing/","title":"Publishing","text":""},{"location":"research/publishing/#adding-a-readmemd-file","title":"Adding a README.md file","text":"<p>Your replication package will need a README file.  We provide a template file, <code>sample-README.md</code>, that you can use as a starting point for your package.  Just rename the file to <code>README.md</code> and edit accordingly. There are several places with \"TODO\" notices that you will definitely want to change.  And we recommend also adding a section at the end that describes each analysis in a bit more detail.</p>"},{"location":"research/publishing/#optional-configuring-docker-support","title":"(optional) Configuring Docker Support","text":"<p>The study template has a sample <code>Dockerfile</code> to build a Python-based container that is capable of building the study.  You will need to update two things. First, update the version of Python used.  Currently, the Dockerfile is set to Python 3.10, so if you used a newer version of Python be sure to update.  Try to use a concrete version (e.g., <code>3.11.2</code>) rather than a generic version. Second, you will want to edit the <code>Makefile</code> to change the name of the generated Docker image.  By default, the image will be named \"study-template\".</p>"},{"location":"research/publishing/#packaging","title":"Packaging","text":"<p>If you run <code>make package</code>, replication package ZIP files will be generated.  It generates three different ZIP files:</p> <ul> <li><code>replication-pkg.zip</code><ul> <li>The main replication package and should include all Boa queries, analysis scripts, support scripts, and additional things like the documentation and Makefile.</li> </ul> </li> <li><code>data-cache.zip</code><ul> <li>The pre-processed Parquet files.  This is the cache of the raw data files, in a compressed binary format.</li> </ul> </li> <li><code>data.zip</code><ul> <li>The contents of the <code>data/txt/</code> folder, which is the raw outputs from Boa.</li> </ul> </li> </ul> <p>Most people simply trying to regenerate the tables/figures will only need the first and second files.  The file with the full data is typically quite large and only needed if someone plans to extend/enhance the analyses somehow.</p> <p>Note that the command updates existing ZIP files if they exist, so you can simply re-run it after small changes to a few files and it will update.  It will not, however, remove any files from the ZIP files so if you wind up deleting a query/analysis you either need to remove the ZIPs with <code>make clean-zip</code> and regenerate, or manually run the <code>zip</code> command and remove file(s) from the generates ZIPs.</p>"},{"location":"research/publishing/#publishing-the-package","title":"Publishing the Package","text":"<p>Once you have build your replication package, you can also upload it to Zenodo. To do so, you will need to create a <code>.env</code> file with the following contents:</p> .env<pre><code>ZENODO_API_TOKEN='&lt;your API token here&gt;'\nZENODO_API_ENDPOINT='https://zenodo.org'\n# or for testing:\nZENODO_API_ENDPOINT='https://sandbox.zenodo.org'\n</code></pre> <p>This requires logging into Zenodo and creating an API token.  Be sure to not share the <code>.env</code> file with anyone once you create it!  It will not be placed into any ZIP files and is ignored by Git, but you will still want to be careful with it.</p> <p>Then select an API endpoint.  If you want to test creating a Zenodo record you can utilize their sandbox server.  Otherwise, use the main server.  Note that if you use the sandbox, you need to create a token on that server, as tokens are not shared across the two servers.</p> <p>To utilize the script, simply run <code>make zenodo</code>.</p> <p>The metadata for the Zenodo record is stored in the <code>.zenodo.json</code> file.  This file can be shared and by default is stored in Git.  If one does not exist, the first time you run the command one will be generated and it will stop processing to allow you time to edit it. This file contains the metadata for your record, including things like the title, description, creators, and license info.  By default, we selected CC-By-4.0 as the license, so feel free to change it if needed.</p> <p>For a double-blinded submission, you will want to ensure the creators are listed as anonymous and the access rights set to \"open\":</p> .zenodo.json<pre><code>    \"creators\": [\n        {\n            \"affiliation\": \"Anonymous\",\n            \"name\": \"Anonymous\"\n        }\n    ],\n    \"access_right\": \"open\",\n</code></pre> <p>After your paper is published, you can update the metadata and re-run the script to have it update the metadata with your actual author name(s).</p> <p>For more details on the metadata JSON format, see this link: https://developers.zenodo.org/#representation</p>"},{"location":"research/templates/","title":"Query Templates","text":"study-config.json<pre><code>  \"substitutions\": [\n    {\n      \"target\": \"{@escape@}\",\n      \"file\": \"escape.boa\"\n    },\n    {\n      \"target\": \"{@project-filter@}\",\n      \"replacement\": \"if (input.stars &gt;= 5)\"\n    }\n  ],\n</code></pre> <p>Query templates are defined through a list of substitutions.  Substitutions are defined using two keys.  The <code>target</code> key is mandatory, containing the text which is replaced in the query.  Targets must start with <code>{@</code> and end with <code>@}</code>.  Target names are restricted to alphanumeric characters, and some special characters like <code>-_:.</code>.  Then, either <code>replacement</code> or <code>file</code> are used, with <code>replacement</code> providing the text directly, and <code>file</code> being the name of a file under <code>boa/snippets/</code> which replaces <code>target</code>.  Care must be taken to ensure that the query is valid Boa code after substitution is completed.</p> <p>Before performing substitutions, a substitutions list is constructed, first from local substitutions then from global substitutions.  If two substitutions define the same <code>target</code>, the first one defined is used.  Substitution will iterate through the substitutions list until a loop has been completed without any substitutions (i.e., a steady state has been reached).</p>"}]}